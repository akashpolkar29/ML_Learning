## **Linear Regression** üìàüë§  

Linear Regression is a **supervised learning algorithm** used to model the relationship between **independent variables (features X)** and **dependent variables (label Y)** by fitting a straight line.  

The mathematical form of **Linear Regression**:  
```math
Y = W X + b
```
Where:  
- \( Y \) = Predicted output (label)  
- \( X \) = Input features  
- \( W \) = Weights (coefficients)  
- \( b \) = Bias (intercept)  

---

## **1Ô∏è‚É£ Ordinary Least Squares (OLS) Method üìè**  
OLS is a method to find the best-fitting line by minimizing the sum of squared errors (residuals).  

The error (residual) for each data point:  
```math
\text{Error} = Y_{\text{actual}} - Y_{\text{predicted}}
```

The **cost function (Mean Squared Error - MSE)** measures the overall error:  
```math
J(W, b) = \frac{1}{n} \sum_{i=1}^{n} (Y_i - (WX_i + b))^2
```
- The goal is to find **W** and **b** that minimize this function.  

### **Python Implementation of OLS**
```python
from sklearn.linear_model import LinearRegression  

# Training Data
X = [[1], [2], [3], [4], [5]]  # Feature (independent variable)
y = [2, 4, 6, 8, 10]           # Label (dependent variable)

# Model Initialization & Training
model = LinearRegression()
model.fit(X, y)

# Predictions
pred = model.predict([[6]])  # Predict for X=6
print(f"Prediction for X=6: {pred[0]:.2f}")  # Output: 12.00
```

---

## **2Ô∏è‚É£ Cost Function (MSE - Mean Squared Error) üéØ**  
The **cost function** is used to measure how far the predicted values are from actual values:  

```math
J(W, b) = \frac{1}{n} \sum (Y_{\text{actual}} - Y_{\text{predicted}})^2
```

- **Smaller MSE** ‚Üí Better Model  
- The objective is to minimize this function.  

### **Python Code for MSE**
```python
from sklearn.metrics import mean_squared_error

y_actual = [2, 4, 6, 8, 10]  # True values
y_predicted = [1.8, 4.1, 5.9, 8.2, 9.8]  # Model predictions

mse = mean_squared_error(y_actual, y_predicted)
print(f"Mean Squared Error: {mse:.4f}")  # Output: Small MSE means better accuracy
```

---

## **3Ô∏è‚É£ Gradient Descent (Optimization Algorithm) üöÄ**  
Gradient Descent is an **iterative optimization** algorithm used to find the best values of **W** and **b** by **minimizing the cost function**.  

### **Gradient Descent Formula**  
```math
W := W - \alpha \frac{\partial J}{\partial W}
```

```math
b := b - \alpha \frac{\partial J}{\partial b}
```

Where:  
- **\( \alpha \)** = Learning rate (step size for updates)  
- **\( \frac{\partial J}{\partial W} \)** and **\( \frac{\partial J}{\partial b} \)** are the gradients (derivatives).  

### **Python Implementation of Gradient Descent**
```python
import numpy as np

# Sample Data
X = np.array([1, 2, 3, 4, 5])  # Features
y = np.array([2, 4, 6, 8, 10]) # Labels

# Initialize parameters
W = 0  # Initial weight
b = 0  # Initial bias
alpha = 0.01  # Learning rate
epochs = 1000  # Number of iterations

# Gradient Descent Algorithm
for _ in range(epochs):
    y_pred = W * X + b  # Predictions
    error = y_pred - y  # Error

    # Compute gradients
    dW = (2 / len(X)) * np.sum(error * X)
    db = (2 / len(X)) * np.sum(error)

    # Update parameters
    W -= alpha * dW
    b -= alpha * db

print(f"Final Weight (W): {W:.4f}, Final Bias (b): {b:.4f}")
```


